# Distributed Training

## Overview of Distributed Training

### What does D.T. gain you?

### Chart of Pros and Cons and Types of Training

## Multi-worker Training

### Pytorch Distributed Data Parallel (DDP)

#### With CUDA

**Setup GPU cluster & initialize the distributed process group**

``` python
def setup(rank, world_size):
    
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12369'

    # initialize the process group
    dist.init_process_group("gloo", rank=rank, world_size=world_size)
```

**Start training by spawning a parallel process on each GPU**

``` python
if __name__ == "__main__":
    
    n_gpus = torch.cuda.device_count()
    assert n_gpus >= 2, f"Requires at least 2 GPUs to run, but got {n_gpus}"
    world_size = n_gpus
    
    mp.spawn(main,
             args=(world_size,),
             nprocs=world_size,
             join=True)
```

#### Without CUDA

### Tensorflow's MultiWorkerMirroredStrategy

**Loading files that can't fit into memory all at once using Python Generators**

If we have a dataset that is too large to load into memory all at once, we'll need to use a *generator*. Generators allow you to return only one value of a loop at a time. So in the code below, the training step will call the generator at each step, but when it hits 'yield', it will return the data the training code. Once the training code calls the generator again, it will pick up at the same place it left off and continue with the for loop, returning the next batch of data. This way, we only ever load one batch data into memory at a time.

``` python
def get_train():
    """ Training data generator """
    for file in train_files:
        img = ( np.array(nc.Dataset(file, "r")["ims"][0:1])[0], np.array(nc.Dataset(file, "r")["migrants"][0:1]) )
        yield img


def get_val():
    """ Validation data generator """
    for file in val_files:
        img = ( np.array(nc.Dataset(file, "r")["ims"][0:1])[0], np.array(nc.Dataset(file, "r")["migrants"][0:1]) )
        yield img
        
```

**Loading generators as TF Datasets**

To be compatible with TF's `model.fit()` method, we need to convert our generators into `tf.data.Dataset()` objects. We do this by calling the constructing on our generator function name and specifying the output types of our batch data.

``` python
IMAGERY_DIR = "/sciclone/scr-mlt/hmbaier/cropped/"
files = os.listdir(IMAGERY_DIR)
files = [IMAGERY_DIR + i for i in files if "484" in i]

print("Number of nCDF files: ", len(files))

train_files, val_files = train_test_split(files, .75)
train_dataset = tf.data.Dataset.from_generator(generator = get_train, output_types=(tf.float32, tf.float32)).batch(int(args.world_size))
val_dataset = tf.data.Dataset.from_generator(generator = get_val, output_types=(tf.float32, tf.float32)).batch(int(args.world_size))

print("Number of training files: ", len(train_files))
print("Number of validation files: ", len(val_files)) 
```

    >>> Number of nCDF files: 
    >>> Number of training files: 
    >>> Number of validation files: 

**Setting up the training cluster using TF_CONFIG**

``` python
ips_direc = "/sciclone/home20/hmbaier/tflow/ips/"
os.environ["TF_CONFIG"] = make_config(int(args.rank) - 1, ips_direc, 45962)


def make_config(rank,  direc, port):
    ips = []
    for file in os.listdir(direc):
        with open(direc + file, "r") as f:
            ips.append(f"{f.read().splitlines()[0]}:{port}")
    config = {"cluster": {"worker": ips}, "task": {"index": int(rank), "type": "worker"}}
    return json.dumps(config)
```

**Initializing the MultiWorkerMirroredStrategy**

``` python
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

with strategy.scope():

    train_dataset = strategy.experimental_distribute_dataset(train_dataset)
    val_dataset = strategy.experimental_distribute_dataset(val_dataset)

    multi_worker_model = resnet.resnet56(img_input = tf.keras.layers.Input((None, None, 3)), classes = 1)

    multi_worker_model.compile(
        optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),
        loss = tf.keras.losses.MeanAbsoluteError(),
        metrics=[tf.keras.losses.MeanAbsoluteError()]
    )
```

**Train the MultiWorkerMirroredStrategy model**

``` python
multi_worker_model.fit(train_dataset,
                    steps_per_epoch = int(len(files) // int(args.world_size)),
                    validation_data = val_dataset)
                        
```


**Replication Code**
Replication code is in the folder:  


### References & Helpful Sites

## Distributed Training in TensorFlow

### tfds & large Tensorflow Datasets

### References & Helpful Sites

## Other Distributed Training Frameworks

### Microsoft's DeepSpeed
