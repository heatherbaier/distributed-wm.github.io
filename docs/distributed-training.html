<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Distributed Training | Distributed ML at W&amp;M</title>
  <meta name="description" content="Resrouces and help for distributed ML on the W&amp;M cluster" />
  <meta name="generator" content="bookdown 0.25 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Distributed Training | Distributed ML at W&amp;M" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Resrouces and help for distributed ML on the W&amp;M cluster" />
  <meta name="github-repo" content="heatherbaier/distributed-wm.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Distributed Training | Distributed ML at W&amp;M" />
  
  <meta name="twitter:description" content="Resrouces and help for distributed ML on the W&amp;M cluster" />
  

<meta name="author" content="Heather Baier" />


<meta name="date" content="2022-03-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mpi-python.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Distributed ML at W&M</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction<span></span></a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction<span></span></a></li>
<li class="chapter" data-level="3" data-path="mpi-python.html"><a href="mpi-python.html"><i class="fa fa-check"></i><b>3</b> MPI &amp; Python<span></span></a>
<ul>
<li class="chapter" data-level="3.1" data-path="mpi-python.html"><a href="mpi-python.html#what-is-mpi"><i class="fa fa-check"></i><b>3.1</b> What is MPI?<span></span></a></li>
<li class="chapter" data-level="3.2" data-path="mpi-python.html"><a href="mpi-python.html#rank-world-size"><i class="fa fa-check"></i><b>3.2</b> Rank &amp; World Size<span></span></a></li>
<li class="chapter" data-level="3.3" data-path="mpi-python.html"><a href="mpi-python.html#types-of-communication"><i class="fa fa-check"></i><b>3.3</b> Types of Communication<span></span></a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="mpi-python.html"><a href="mpi-python.html#overview-and-difference-between-p2p-and-cc---you-see-them-everywhere"><i class="fa fa-check"></i><b>3.3.1</b> Overview and difference between P2P and CC - you see them everywhere<span></span></a></li>
<li class="chapter" data-level="3.3.2" data-path="mpi-python.html"><a href="mpi-python.html#block-non-blocking"><i class="fa fa-check"></i><b>3.3.2</b> Block / non-blocking<span></span></a></li>
<li class="chapter" data-level="3.3.3" data-path="mpi-python.html"><a href="mpi-python.html#point-to-point-communication"><i class="fa fa-check"></i><b>3.3.3</b> Point-to-Point Communication<span></span></a></li>
<li class="chapter" data-level="3.3.4" data-path="mpi-python.html"><a href="mpi-python.html#collective-communication"><i class="fa fa-check"></i><b>3.3.4</b> Collective Communication<span></span></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="mpi-python.html"><a href="mpi-python.html#references-helpful-sites"><i class="fa fa-check"></i><b>3.4</b> References &amp; Helpful Sites<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="distributed-training.html"><a href="distributed-training.html"><i class="fa fa-check"></i><b>4</b> Distributed Training<span></span></a>
<ul>
<li class="chapter" data-level="4.1" data-path="distributed-training.html"><a href="distributed-training.html#overview-of-distributed-training"><i class="fa fa-check"></i><b>4.1</b> Overview of Distributed Training<span></span></a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="distributed-training.html"><a href="distributed-training.html#what-does-d.t.-gain-you"><i class="fa fa-check"></i><b>4.1.1</b> What does D.T. gain you?<span></span></a></li>
<li class="chapter" data-level="4.1.2" data-path="distributed-training.html"><a href="distributed-training.html#chart-of-pros-and-cons-and-types-of-training"><i class="fa fa-check"></i><b>4.1.2</b> Chart of Pros and Cons and Types of Training<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="distributed-training.html"><a href="distributed-training.html#multi-worker-training"><i class="fa fa-check"></i><b>4.2</b> Multi-worker Training<span></span></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="distributed-training.html"><a href="distributed-training.html#pytorch-distributed-data-parallel-ddp"><i class="fa fa-check"></i><b>4.2.1</b> Pytorch Distributed Data Parallel (DDP)<span></span></a></li>
<li class="chapter" data-level="4.2.2" data-path="distributed-training.html"><a href="distributed-training.html#tensorflows-multiworkermirroredstrategy"><i class="fa fa-check"></i><b>4.2.2</b> Tensorflowâ€™s <code>MultiWorkerMirroredStrategy()</code><span></span></a></li>
<li class="chapter" data-level="4.2.3" data-path="distributed-training.html"><a href="distributed-training.html#references-helpful-sites-1"><i class="fa fa-check"></i><b>4.2.3</b> References &amp; Helpful Sites<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="distributed-training.html"><a href="distributed-training.html#other-distributed-training-frameworks"><i class="fa fa-check"></i><b>4.3</b> Other Distributed Training Frameworks<span></span></a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="distributed-training.html"><a href="distributed-training.html#microsofts-deepspeed"><i class="fa fa-check"></i><b>4.3.1</b> Microsoftâ€™s DeepSpeed<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="references.html#references">References<span></span></a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Distributed ML at W&amp;M</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="distributed-training" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Distributed Training<a href="distributed-training.html#distributed-training" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="overview-of-distributed-training" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Overview of Distributed Training<a href="distributed-training.html#overview-of-distributed-training" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="what-does-d.t.-gain-you" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> What does D.T. gain you?<a href="distributed-training.html#what-does-d.t.-gain-you" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="chart-of-pros-and-cons-and-types-of-training" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Chart of Pros and Cons and Types of Training<a href="distributed-training.html#chart-of-pros-and-cons-and-types-of-training" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
</div>
<div id="multi-worker-training" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Multi-worker Training<a href="distributed-training.html#multi-worker-training" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="pytorch-distributed-data-parallel-ddp" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Pytorch Distributed Data Parallel (DDP)<a href="distributed-training.html#pytorch-distributed-data-parallel-ddp" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="with-cuda" class="section level4 hasAnchor" number="4.2.1.1">
<h4><span class="header-section-number">4.2.1.1</span> With CUDA<a href="distributed-training.html#with-cuda" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Setup GPU cluster &amp; initialize the distributed process group</strong></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="distributed-training.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> setup(rank, world_size):</span>
<span id="cb3-2"><a href="distributed-training.html#cb3-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-3"><a href="distributed-training.html#cb3-3" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">&#39;MASTER_ADDR&#39;</span>] <span class="op">=</span> <span class="st">&#39;localhost&#39;</span></span>
<span id="cb3-4"><a href="distributed-training.html#cb3-4" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">&#39;MASTER_PORT&#39;</span>] <span class="op">=</span> <span class="st">&#39;12369&#39;</span></span>
<span id="cb3-5"><a href="distributed-training.html#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="distributed-training.html#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize the process group</span></span>
<span id="cb3-7"><a href="distributed-training.html#cb3-7" aria-hidden="true" tabindex="-1"></a>    dist.init_process_group(<span class="st">&quot;gloo&quot;</span>, rank<span class="op">=</span>rank, world_size<span class="op">=</span>world_size)</span></code></pre></div>
<p><strong>Start training by spawning a parallel process on each GPU</strong></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="distributed-training.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>:</span>
<span id="cb4-2"><a href="distributed-training.html#cb4-2" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-3"><a href="distributed-training.html#cb4-3" aria-hidden="true" tabindex="-1"></a>    n_gpus <span class="op">=</span> torch.cuda.device_count()</span>
<span id="cb4-4"><a href="distributed-training.html#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> n_gpus <span class="op">&gt;=</span> <span class="dv">2</span>, <span class="ss">f&quot;Requires at least 2 GPUs to run, but got </span><span class="sc">{</span>n_gpus<span class="sc">}</span><span class="ss">&quot;</span></span>
<span id="cb4-5"><a href="distributed-training.html#cb4-5" aria-hidden="true" tabindex="-1"></a>    world_size <span class="op">=</span> n_gpus</span>
<span id="cb4-6"><a href="distributed-training.html#cb4-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-7"><a href="distributed-training.html#cb4-7" aria-hidden="true" tabindex="-1"></a>    mp.spawn(main,</span>
<span id="cb4-8"><a href="distributed-training.html#cb4-8" aria-hidden="true" tabindex="-1"></a>             args<span class="op">=</span>(world_size,),</span>
<span id="cb4-9"><a href="distributed-training.html#cb4-9" aria-hidden="true" tabindex="-1"></a>             nprocs<span class="op">=</span>world_size,</span>
<span id="cb4-10"><a href="distributed-training.html#cb4-10" aria-hidden="true" tabindex="-1"></a>             join<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
</div>
<div id="without-cuda" class="section level4 hasAnchor" number="4.2.1.2">
<h4><span class="header-section-number">4.2.1.2</span> Without CUDA<a href="distributed-training.html#without-cuda" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
</div>
<div id="tensorflows-multiworkermirroredstrategy" class="section level3 hasAnchor" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Tensorflowâ€™s <code>MultiWorkerMirroredStrategy()</code><a href="distributed-training.html#tensorflows-multiworkermirroredstrategy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Loading files that canâ€™t fit into memory all at once using Python Generators</strong></p>
<p>If we have a dataset that is too large to load into memory all at once, weâ€™ll need to use a <em>generator</em>. Generators allow you to return only one value of a loop at a time. So in the code below, the training step will call the generator at each step, but when it hits â€˜yieldâ€™, it will return the data the training code. Once the training code calls the generator again, it will pick up at the same place it left off and continue with the for loop, returning the next batch of data. This way, we only ever load one batch data into memory at a time.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="distributed-training.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_train():</span>
<span id="cb5-2"><a href="distributed-training.html#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot; Training data generator &quot;&quot;&quot;</span></span>
<span id="cb5-3"><a href="distributed-training.html#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="bu">file</span> <span class="kw">in</span> train_files:</span>
<span id="cb5-4"><a href="distributed-training.html#cb5-4" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> ( np.array(nc.Dataset(<span class="bu">file</span>, <span class="st">&quot;r&quot;</span>)[<span class="st">&quot;ims&quot;</span>][<span class="dv">0</span>:<span class="dv">1</span>])[<span class="dv">0</span>], np.array(nc.Dataset(<span class="bu">file</span>, <span class="st">&quot;r&quot;</span>)[<span class="st">&quot;migrants&quot;</span>][<span class="dv">0</span>:<span class="dv">1</span>]) )</span>
<span id="cb5-5"><a href="distributed-training.html#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">yield</span> img</span>
<span id="cb5-6"><a href="distributed-training.html#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="distributed-training.html#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="distributed-training.html#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_val():</span>
<span id="cb5-9"><a href="distributed-training.html#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot; Validation data generator &quot;&quot;&quot;</span></span>
<span id="cb5-10"><a href="distributed-training.html#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="bu">file</span> <span class="kw">in</span> val_files:</span>
<span id="cb5-11"><a href="distributed-training.html#cb5-11" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> ( np.array(nc.Dataset(<span class="bu">file</span>, <span class="st">&quot;r&quot;</span>)[<span class="st">&quot;ims&quot;</span>][<span class="dv">0</span>:<span class="dv">1</span>])[<span class="dv">0</span>], np.array(nc.Dataset(<span class="bu">file</span>, <span class="st">&quot;r&quot;</span>)[<span class="st">&quot;migrants&quot;</span>][<span class="dv">0</span>:<span class="dv">1</span>]) )</span>
<span id="cb5-12"><a href="distributed-training.html#cb5-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">yield</span> img</span>
<span id="cb5-13"><a href="distributed-training.html#cb5-13" aria-hidden="true" tabindex="-1"></a>        </span></code></pre></div>
<p><strong>Loading generators as TF Datasets</strong></p>
<p>To be compatible with TFâ€™s <code>model.fit()</code> method, we need to convert our generators into <code>tf.data.Dataset()</code> objects. We do this by calling the constructing on our generator function name and specifying the output types of our batch data.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="distributed-training.html#cb6-1" aria-hidden="true" tabindex="-1"></a>IMAGERY_DIR <span class="op">=</span> <span class="st">&quot;/sciclone/scr-mlt/hmbaier/cropped/&quot;</span></span>
<span id="cb6-2"><a href="distributed-training.html#cb6-2" aria-hidden="true" tabindex="-1"></a>files <span class="op">=</span> os.listdir(IMAGERY_DIR)</span>
<span id="cb6-3"><a href="distributed-training.html#cb6-3" aria-hidden="true" tabindex="-1"></a>files <span class="op">=</span> [IMAGERY_DIR <span class="op">+</span> i <span class="cf">for</span> i <span class="kw">in</span> files <span class="cf">if</span> <span class="st">&quot;484&quot;</span> <span class="kw">in</span> i]</span>
<span id="cb6-4"><a href="distributed-training.html#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="distributed-training.html#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Number of nCDF files: &quot;</span>, <span class="bu">len</span>(files))</span>
<span id="cb6-6"><a href="distributed-training.html#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="distributed-training.html#cb6-7" aria-hidden="true" tabindex="-1"></a>train_files, val_files <span class="op">=</span> train_test_split(files, <span class="fl">.75</span>)</span>
<span id="cb6-8"><a href="distributed-training.html#cb6-8" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> tf.data.Dataset.from_generator(generator <span class="op">=</span> get_train, output_types<span class="op">=</span>(tf.float32, tf.float32)).batch(<span class="bu">int</span>(args.world_size))</span>
<span id="cb6-9"><a href="distributed-training.html#cb6-9" aria-hidden="true" tabindex="-1"></a>val_dataset <span class="op">=</span> tf.data.Dataset.from_generator(generator <span class="op">=</span> get_val, output_types<span class="op">=</span>(tf.float32, tf.float32)).batch(<span class="bu">int</span>(args.world_size))</span>
<span id="cb6-10"><a href="distributed-training.html#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="distributed-training.html#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Number of training files: &quot;</span>, <span class="bu">len</span>(train_files))</span>
<span id="cb6-12"><a href="distributed-training.html#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Number of validation files: &quot;</span>, <span class="bu">len</span>(val_files)) </span></code></pre></div>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="distributed-training.html#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> Number of nCDF files: </span>
<span id="cb7-2"><a href="distributed-training.html#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> Number of training files: </span>
<span id="cb7-3"><a href="distributed-training.html#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> Number of validation files: </span></code></pre></div>
<p><strong>Initializing the <code>MultiWorkerMirroredStrategy()</code></strong></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="distributed-training.html#cb8-1" aria-hidden="true" tabindex="-1"></a>strategy <span class="op">=</span> tf.distribute.experimental.MultiWorkerMirroredStrategy()</span>
<span id="cb8-2"><a href="distributed-training.html#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="distributed-training.html#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> strategy.scope():</span>
<span id="cb8-4"><a href="distributed-training.html#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="distributed-training.html#cb8-5" aria-hidden="true" tabindex="-1"></a>    train_dataset <span class="op">=</span> strategy.experimental_distribute_dataset(train_dataset)</span>
<span id="cb8-6"><a href="distributed-training.html#cb8-6" aria-hidden="true" tabindex="-1"></a>    val_dataset <span class="op">=</span> strategy.experimental_distribute_dataset(val_dataset)</span>
<span id="cb8-7"><a href="distributed-training.html#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="distributed-training.html#cb8-8" aria-hidden="true" tabindex="-1"></a>    multi_worker_model <span class="op">=</span> resnet.resnet56(img_input <span class="op">=</span> tf.keras.layers.Input((<span class="va">None</span>, <span class="va">None</span>, <span class="dv">3</span>)), classes <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb8-9"><a href="distributed-training.html#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="distributed-training.html#cb8-10" aria-hidden="true" tabindex="-1"></a>    multi_worker_model.<span class="bu">compile</span>(</span>
<span id="cb8-11"><a href="distributed-training.html#cb8-11" aria-hidden="true" tabindex="-1"></a>        optimizer <span class="op">=</span> tf.keras.optimizers.Adam(learning_rate <span class="op">=</span> <span class="fl">0.001</span>),</span>
<span id="cb8-12"><a href="distributed-training.html#cb8-12" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> tf.keras.losses.MeanAbsoluteError(),</span>
<span id="cb8-13"><a href="distributed-training.html#cb8-13" aria-hidden="true" tabindex="-1"></a>        metrics<span class="op">=</span>[tf.keras.losses.MeanAbsoluteError()]</span>
<span id="cb8-14"><a href="distributed-training.html#cb8-14" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div>
<p><strong>Train the <code>MultiWorkerMirroredStrategy()</code> model</strong></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="distributed-training.html#cb9-1" aria-hidden="true" tabindex="-1"></a>multi_worker_model.fit(train_dataset,</span>
<span id="cb9-2"><a href="distributed-training.html#cb9-2" aria-hidden="true" tabindex="-1"></a>                    steps_per_epoch <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(files) <span class="op">//</span> <span class="bu">int</span>(args.world_size)),</span>
<span id="cb9-3"><a href="distributed-training.html#cb9-3" aria-hidden="true" tabindex="-1"></a>                    validation_data <span class="op">=</span> val_dataset)</span>
<span id="cb9-4"><a href="distributed-training.html#cb9-4" aria-hidden="true" tabindex="-1"></a>                        </span></code></pre></div>
<div id="launching-a-tensorflow-multiworkermirroredstrategy-script" class="section level4 hasAnchor" number="4.2.2.1">
<h4><span class="header-section-number">4.2.2.1</span> Launching a Tensorflow <code>MultiWorkerMirroredStrategy()</code> script<a href="distributed-training.html#launching-a-tensorflow-multiworkermirroredstrategy-script" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Launching a multi-worker training job is a little more intricate than simply calling <code>python train.py</code>. In order to set up our cluster, we need to know the IP addresses of each node in our job, then we need to launch the training scipt on each node individually. Weâ€™ll use a set of bash scripts to do this.</p>
<p><strong>run.sh</strong></p>
<p>The rm commands inthis script delete files from any previous runs. The purpose of this script is to qsub the <code>tflow_job.sh</code> job on the number of nodes that you want to include in training. This is the only scipt youâ€™ll run to launch your training. You can use it by running: <code>./run.sh &lt;NUM_NODES&gt;</code></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb10-1"><a href="distributed-training.html#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span> <span class="at">-R</span> /sciclone/home20/hmbaier/tflow/ips/</span>
<span id="cb10-2"><a href="distributed-training.html#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> /sciclone/home20/hmbaier/tflow/ips/</span>
<span id="cb10-3"><a href="distributed-training.html#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="distributed-training.html#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span> <span class="at">-R</span> /sciclone/home20/hmbaier/tflow/logs/</span>
<span id="cb10-5"><a href="distributed-training.html#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> /sciclone/home20/hmbaier/tflow/logs/</span>
<span id="cb10-6"><a href="distributed-training.html#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="distributed-training.html#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="kw">((</span><span class="va">i</span> <span class="op">=</span> <span class="dv">1</span><span class="kw">;</span> <span class="va">i</span> <span class="op">&lt;=</span> <span class="va">$1</span><span class="kw">;</span> <span class="va">i</span><span class="op">++</span><span class="kw">))</span></span>
<span id="cb10-8"><a href="distributed-training.html#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="cf">do</span></span>
<span id="cb10-9"><a href="distributed-training.html#cb10-9" aria-hidden="true" tabindex="-1"></a>    <span class="ex">qsub</span> /sciclone/home20/hmbaier/tflow/tflow_job.sh <span class="at">-l</span> nodes=1:vortex:ppn=1 <span class="at">-v</span> NODE_NUM=<span class="va">$i</span>,WORLD_SIZE=<span class="va">$1</span></span>
<span id="cb10-10"><a href="distributed-training.html#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="cf">done</span></span></code></pre></div>
<div class="sourceCode" id="cb11"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb11-1"><a href="distributed-training.html#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co">#!/bin/tcsh</span></span>
<span id="cb11-2"><a href="distributed-training.html#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -N head_node</span></span>
<span id="cb11-3"><a href="distributed-training.html#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -l walltime=10:00:00</span></span>
<span id="cb11-4"><a href="distributed-training.html#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">#PBS -j oe</span></span>
<span id="cb11-5"><a href="distributed-training.html#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="distributed-training.html#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="distributed-training.html#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="fu">hostname</span> <span class="at">-i</span> <span class="op">&gt;</span> /sciclone/home20/hmbaier/tflow/ips/<span class="va">$NODE_NUM</span>.txt</span>
<span id="cb11-8"><a href="distributed-training.html#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="distributed-training.html#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="bu">set</span> size=<span class="kw">`</span><span class="fu">ls</span> /sciclone/home20/hmbaier/tflow/ips/ <span class="kw">|</span> <span class="fu">wc</span> <span class="at">-l</span><span class="kw">`</span></span>
<span id="cb11-10"><a href="distributed-training.html#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="distributed-training.html#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="kw">(</span> <span class="va">$size</span> != <span class="va">$WORLD_SIZE</span> <span class="kw">)</span></span>
<span id="cb11-12"><a href="distributed-training.html#cb11-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">set</span> size=<span class="kw">`</span><span class="fu">ls</span> /sciclone/home20/hmbaier/tflow/ips/ <span class="kw">|</span> <span class="fu">wc</span> <span class="at">-l</span><span class="kw">`</span></span>
<span id="cb11-13"><a href="distributed-training.html#cb11-13" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sleep</span> 1</span>
<span id="cb11-14"><a href="distributed-training.html#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="ex">end</span></span>
<span id="cb11-15"><a href="distributed-training.html#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="distributed-training.html#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">&quot;</span><span class="va">$size</span><span class="st">&quot;</span></span>
<span id="cb11-17"><a href="distributed-training.html#cb11-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-18"><a href="distributed-training.html#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="distributed-training.html#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="co"># init conda within new shell for job</span></span>
<span id="cb11-20"><a href="distributed-training.html#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> <span class="st">&quot;/usr/local/anaconda3-2021.05/etc/profile.d/conda.csh&quot;</span></span>
<span id="cb11-21"><a href="distributed-training.html#cb11-21" aria-hidden="true" tabindex="-1"></a><span class="ex">module</span> load anaconda3/2021.05</span>
<span id="cb11-22"><a href="distributed-training.html#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="ex">unsetenv</span> PYTHONPATH</span>
<span id="cb11-23"><a href="distributed-training.html#cb11-23" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> activate tflow</span>
<span id="cb11-24"><a href="distributed-training.html#cb11-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-25"><a href="distributed-training.html#cb11-25" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> /sciclone/home20/hmbaier/tflow/worker_v5.py <span class="va">$NODE_NUM</span> <span class="va">$WORLD_SIZE</span> <span class="op">&gt;</span> <span class="st">&quot;/sciclone/home20/hmbaier/tflow/logs/log</span><span class="va">${NODE_NUM}</span><span class="st">.txt&quot;</span></span></code></pre></div>
<p><strong>Connecting to the Python script: Setting up the training cluster using TF_CONFIG</strong></p>
<p>The tflow_job you submitted through <code>run.sh</code> will call a file called <code>ips.sh</code> that will save a text file with thte IP address of each node that will participate in training. At the beginning of our python training script, we will use this make_config function to format those IP address into the environment variable Tensorflow needs to set up itâ€™s training cluster.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="distributed-training.html#cb12-1" aria-hidden="true" tabindex="-1"></a>ips_direc <span class="op">=</span> <span class="st">&quot;/sciclone/home20/hmbaier/tflow/ips/&quot;</span></span>
<span id="cb12-2"><a href="distributed-training.html#cb12-2" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">&quot;TF_CONFIG&quot;</span>] <span class="op">=</span> make_config(<span class="bu">int</span>(args.rank) <span class="op">-</span> <span class="dv">1</span>, ips_direc, <span class="dv">45962</span>)</span>
<span id="cb12-3"><a href="distributed-training.html#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="distributed-training.html#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="distributed-training.html#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_config(rank,  direc, port):</span>
<span id="cb12-6"><a href="distributed-training.html#cb12-6" aria-hidden="true" tabindex="-1"></a>    ips <span class="op">=</span> []</span>
<span id="cb12-7"><a href="distributed-training.html#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="bu">file</span> <span class="kw">in</span> os.listdir(direc):</span>
<span id="cb12-8"><a href="distributed-training.html#cb12-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="bu">open</span>(direc <span class="op">+</span> <span class="bu">file</span>, <span class="st">&quot;r&quot;</span>) <span class="im">as</span> f:</span>
<span id="cb12-9"><a href="distributed-training.html#cb12-9" aria-hidden="true" tabindex="-1"></a>            ips.append(<span class="ss">f&quot;</span><span class="sc">{</span>f<span class="sc">.</span>read()<span class="sc">.</span>splitlines()[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>port<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb12-10"><a href="distributed-training.html#cb12-10" aria-hidden="true" tabindex="-1"></a>    config <span class="op">=</span> {<span class="st">&quot;cluster&quot;</span>: {<span class="st">&quot;worker&quot;</span>: ips}, <span class="st">&quot;task&quot;</span>: {<span class="st">&quot;index&quot;</span>: <span class="bu">int</span>(rank), <span class="st">&quot;type&quot;</span>: <span class="st">&quot;worker&quot;</span>}}</span>
<span id="cb12-11"><a href="distributed-training.html#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> json.dumps(config)</span></code></pre></div>
<p><strong>Replication Code</strong></p>
<p>Replication code is in the folder: <a href="https://github.com/heatherbaier/distributed-wm.github.io/tree/gh-pages/examples/tf_mwms" class="uri">https://github.com/heatherbaier/distributed-wm.github.io/tree/gh-pages/examples/tf_mwms</a></p>
<p>To launch the training script, open up a terminal and ssh into Vortex.</p>
<p>If youâ€™ve never done the tutorial before, copy the below into your terminal and hit enter.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb13-1"><a href="distributed-training.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> tf_mwms</span>
<span id="cb13-2"><a href="distributed-training.html#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> tf_mwms</span>
<span id="cb13-3"><a href="distributed-training.html#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="distributed-training.html#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> init .</span>
<span id="cb13-5"><a href="distributed-training.html#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> remote add <span class="at">-f</span> origin https://github.com/heatherbaier/distributed-wm.github.io.git</span>
<span id="cb13-6"><a href="distributed-training.html#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> config core.sparseCheckout true</span>
<span id="cb13-7"><a href="distributed-training.html#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="distributed-training.html#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">&quot;examples/tf_mwms/&quot;</span> <span class="op">&gt;&gt;</span> .git/info/sparse-checkout</span>
<span id="cb13-9"><a href="distributed-training.html#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="distributed-training.html#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> pull origin master</span>
<span id="cb13-11"><a href="distributed-training.html#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> checkout gh-pages</span>
<span id="cb13-12"><a href="distributed-training.html#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="distributed-training.html#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> examples/tf_mwms</span>
<span id="cb13-14"><a href="distributed-training.html#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="distributed-training.html#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="fu">chmod</span> +x run.sh</span>
<span id="cb13-16"><a href="distributed-training.html#cb13-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-17"><a href="distributed-training.html#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="ex">./run.sh</span> <span class="op">&lt;</span>NUM_NODES<span class="op">&gt;</span></span></code></pre></div>
<p>If youâ€™ve done the tutorial before, simply cd into examples/tf_mwms and run:</p>
<pre><code>./run.sh &lt;NUM_NODES&gt;</code></pre>
</div>
</div>
<div id="references-helpful-sites-1" class="section level3 hasAnchor" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> References &amp; Helpful Sites<a href="distributed-training.html#references-helpful-sites-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
</div>
<div id="other-distributed-training-frameworks" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Other Distributed Training Frameworks<a href="distributed-training.html#other-distributed-training-frameworks" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="microsofts-deepspeed" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Microsoftâ€™s DeepSpeed<a href="distributed-training.html#microsofts-deepspeed" class="anchor-section" aria-label="Anchor link to header"></a></h3>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mpi-python.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/03-dist_training.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
